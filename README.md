# HCC_FINAL_2022 Implementation 

team: 黃彥菱、吳佳真、李維釗

## BCI

- Attention (tv monitor)
    - 利用Frontal腦區的alpha power與theta power比例來區分專心與否。輕微專注與極專注的threshold設在100以下和5以下。
- Blink (teddy bear, keyboard)
    - 針對Fp1的實時訊號微分，取微分後值為0的轉折點，再以prominence 150與width 50進行篩選，最後算其個數即為眨眼後造成的peak數量，數量偵測設定在4個以上與2~4個。
- Close eyes (bottle)
    - 由於人在閉眼後，Occipital腦區的alpha power會提高，而在測試後得到在O1、O2兩個channels的alpha power平均值在有無閉眼的兩情況大約以10做為分界，因此threshold以此數值作為最後決定。
- 最後區辨先判斷眨眼、輕微專注、極專注，分辨tv monitor和bottle。
- 而在輕微專注的部分在分為眨4下以上和眨2~4下以分辨teddy bear和keyboard。

## ROS

- 主要分成3個檔案實作
    - 依序是 apriltag_localization_2022.cpp 做 tag 位置的定位
    - drone_object.py 物體位置的定位
    - count.py 計算最後的輸出
- 由於已知要看到 tag node 才會 publish 出去，我們就以這個前提去實作本次實驗。
    - **apriltag_localization_2022.cpp**
        1. TF = tf1 * tf_cam * tf2
            
            ```python
            localization_trans = (tag_transforms[id] * min_distance_trans.inverse() * camera_transform);
            ```
            
        2. 看到tag時的機制：
            - 如果是一個全新沒看過的tag，在當前目錄直接新增一個 `<tag_id>.txt` (讓之後py檔的資料寫入)
    - **drone_object.py**
        1. 得到真實座標的轉換：
            - global_transform * 根據相機鏡頭算出的 xyz pose → 物件距離 origin 的 xyz
            
            ```python
            xyz = [v1[0]/1000, v1[1]/1000, v1[2]/1000, 1]
            xyz = np.array(xyz).reshape(4,1)
            trans = global_transform.dot(xyz)
            ```
            
        2. 數據收集的機制：
            - 中間只要有回傳看到物件，我們就會將每一筆 xyz 資料以 list 的形式存起來
        3. 數據儲存的機制：
            - 當看到一個新的 tag 的時候，把定位四個物件的 list 資料寫到前一個 tag 的 txt，並清空 list，以儲存新的 tag 附近的物件定位
        4. tag 定位輸出檔案範例：
            - 每一個 txt 檔都會有四個物件每一次偵測到的 xyz 資訊
            - `1.txt`範例
                
                ```python
                tvmonitor_vec
                [[1.36388428 3.14126361 0.45806315]
                 [1.76802145 4.19244885 1.03752749]
                 [1.82240253 4.00445222 0.91638232]
                 [1.64604005 4.05661004 0.19043209]
                 [1.65765021 4.05570329 0.30383871]
                 [1.74645765 4.0696814  0.20386   ]
                 [1.70765528 3.90370373 0.2817294 ]
                --------------------------
                bottle_vec
                [[2.56739339 1.01531538 0.88391534]
                 [2.11049348 3.84891047 0.13626005]
                 [2.10496012 3.84260076 0.13692419]
                 [2.10174575 3.82904946 0.14192685]
                 [2.08349059 3.83436833 0.14675166]
                 [2.10062107 3.84730569 0.13319285]
                --------------------------
                teddy_bear_vec
                [[ 1.45953648  3.59111719  0.11303215]
                 [ 1.48085588  3.57374004  0.12193376]
                 [ 1.47838024  3.58359676  0.12541433]
                 [ 1.47108284  3.59673149  0.12699698]
                 [ 1.48258707  3.59603834  0.12027036]
                 [ 1.4905105   3.5792238   0.11812916]
                 [ 1.49516749  3.6007851   0.12216626]
                 [ 1.44229601  3.59302764  0.12728859]
                 [ 1.30443148  3.76463862  0.06164306]
                --------------------------
                keyboard_vec
                [[2.36883266e+00 4.15666583e+00 8.47290000e-02]
                 [2.37467482e+00 4.15181127e+00 8.94146797e-02]
                 [2.36576933e+00 4.14951923e+00 8.58819334e-02]
                 [2.40667418e+00 4.15672311e+00 8.62825010e-02]
                 [2.35929622e+00 4.16390317e+00 5.03000042e-02]
                 [2.35779100e+00 4.19477687e+00 4.55308665e-02]
                 [2.39182070e+00 4.28094826e+00 6.33703924e-04]
                 [2.57825507e+00 3.54304852e+00 2.17171120e-01]]
                ```
                
            - 最後整個 bag 執行完後 執行 count.py
    - **count.py**
        1. 依序讀取 4 個 tag 所產生的 txt 檔
        2. 物體位置計算方式：
            - 計算所有同 tag 同物件得到數據的 mean 與std
            - 保留 mean±2*std的數據後重新計算平均值作為同 tag id 名稱 txt 檔之輸出。
        3. 最後讀取 EEG.txt 確定每一個tag 需要定位的物件後，將該資料寫成一個新的 output.txt
